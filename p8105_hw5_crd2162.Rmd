---
title: "p8105_hw5_crd2162"
author: "Caleigh Dwyer"
date: "2023-11-13"
output: github_document
---


```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(stringr)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides. 

## Problem 2: Experimental data

The following chunk uses a function to read multiple csv files from a zip folder. Each .csv file represents longitudinal data for either a control or experimental group subject.

```{r, message = FALSE}
zip_data =
list.files(path = "./data/data_csv", full.names = TRUE)

#example
subject_data = 
    read_csv("./data/data_csv/con_01.csv") |> 
    janitor::clean_names()
  
# function 
read_function = function(filename) {
  subject_data = 
    read_csv(filename) |> 
    janitor::clean_names()
  return(subject_data)
}

#map

table = map(zip_data, ~ read_function(.x))

all_data = 
  tibble(subject = zip_data, subject_data = table) |> 
  unnest(cols = c(subject_data))

```

The combined dataset containing data for all subjects will be tidied in the following chunk.The subject ID was pulled from the file name, creating a new column called "id". The experimental group was then pulled from the "id," creating a new column called "group". 

```{r}
tidy_data =
  all_data |> 
  mutate(
    id = sub(".csv", "", basename(subject)),
    group = ifelse(str_detect(id, "^exp"), "experimental", "control")) |> 
  pivot_longer(week_1:week_8,
               names_to = "time_point",
               values_to = "observation") |> 
  select(id, group, everything(), -subject)
```

A spaghetti plot is created in the chunk below showing observations on each subject over time by group. This graph shows that the experimental group had greater change in the value of the observation over time, as the value of the observation at week 8 is higher among all experimental group subjects compared to control group, even thought experimental and control group subjects had simillar observation values at baseline.

```{r}
subject_graph =
  tidy_data |> 
  ggplot(aes(x = time_point, y = observation, group = id, color = group))+
  geom_point()+
  geom_line()+
  labs(title = "Changes in observation by subject and group over 8 weeks", x = "Week", y = "Observation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(subject_graph)
```

## Problem 3: Simulation

```{r}
set.seed(1)

#write a function to define parameters
sim_power = function(n = 30, sigma = 5, mu = 0){
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
  
  return(sim_data)
}


#create dataset including 5000 iterations of function above
sim_results =
  expand_grid(
    n = 30,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(n, sim_power)
  )

#write function to run t test. Mu can be changed.
sim_power_repeat = function(mu) {
  df = expand_grid(
    n = 30,
    iter = 1:5000
  ) |> 
  mutate(
    mean = mu,
    estimate_df = map(n, sim_power),
    estimates = map(estimate_df, ~ broom::tidy(t.test(.x, mu = mu)))
  ) |> 
  unnest(c(estimates))
  return(df)
}

#run t test function nfor 7 different mu and combine outputs
sim_results_df =   
  map(0:6, ~ sim_power_repeat(.x)) |> 
  bind_rows()



```

