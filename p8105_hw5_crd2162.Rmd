---
title: "p8105_hw5_crd2162"
author: "Caleigh Dwyer"
date: "2023-11-13"
output: github_document
---


```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(stringr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1: Homicide Data

The chunk below loads data from the Washington Post on homicide trends in the 50 largest US cities. 

```{r load_murder_data}
murder_data =
  read_csv("./data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  select(-uid, -victim_last, -victim_first) |> 
  unite(city_state, city, state, sep = ", ") |> 
  mutate(city_state = 
           if_else(
             city_state == "Tulsa, AL", "Tulsa, OK", city_state
           ))
```


The dataset contains information on `r murder_data |> nrow()` criminal homicides reported from 2007 to 2015. The dataset describes the race, age, and sex of each victim, the city and state in which the homicide occurred, the exact latitude and longitude of the crime, and the "disposition" of the crime, which describes whether the case is open or closed and whether an arrest was made. One homicide was misattributed to Tulsa, Alabama when the latitude and longitude indicate that the crime occurred in Oklahoma; this was corrected in the cleaned dataset.

The following chunk summarizes the total number of homicides in each city. Chicago, Philadelphia, Houston, Baltimore, and Detroit had the highest number of total homicides, in that order.

```{r total_murders}
murder_data |> 
  count(city_state) |> 
  arrange(desc(n)) |> 
  knitr::kable()
```

The following chunk summarizes the total number of unsolved homicides in each city, or those in which the disposition is "Closed without arrest" or "Open/No arrest". Chicago, Baltimore, Houston, Detroit, and Philadelphia had the highest number of unsolved homicides, in that order.

```{r}
murder_data |> 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) |> 
  count(city_state) |> 
  arrange(desc(n)) |> 
  knitr::kable()
```

We will now use the prop.test function to estimate the proportion of homicides that are unsolved in Baltimore, MD. The estimated proportion of unsolved homicides in Baltimore was 64.6% with a confidence interval of 62.8%-66.3%

```{r}
prop_bmd =
murder_data |> 
  filter(city_state == "Baltimore, MD") |> 
  summarize(
    total_murder = n(),
    total_unsolved = sum(
           disposition %in% c("Closed without arrest", "Open/No arrest")
         )) |> 
  with(prop.test(total_unsolved,total_murder)) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)

print(prop_bmd)

```

The chunk below creates a function that applies the prop.test used above, producing the estimated proportion of unsolved murders for each input along with the confidence interval. Then, map is used to pass a list of the 50 cities through the function, producing the output for each city. The outputs are then consolidated into a tibble.

```{r}
# function 
prop_function = function(dataset, name) {
  prop = dataset |> 
  filter(city_state == name) |> 
  summarize(
    total_murder = n(),
    total_unsolved = sum(
           disposition %in% c("Closed without arrest", "Open/No arrest")
         )) |> 
  with(prop.test(total_unsolved,total_murder)) |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high)
  return(prop)
}

cities = unique(murder_data$city_state)
table = map(cities, ~ prop_function(murder_data, .x))

# Using list columns
# Unnesting the list column
prob_unsolved = 
  tibble(city = cities, prop = table) |> 
  unnest(cols = c(prop))
```

The chunk below produces a graph showing the estimated proportion of unsolved murders for each city (on a scale of 0 to 1) along with corresponding confidence intervals. The graph shows that Chicago, New Orleans, Baltimore, San Bernardino, and Buffalo had the five highest proportions of unsolved murders. Notably, the confidence interval for the proportion of unsolved murders was very narrow for Chicago and much wider for San Bernardino and Buffalo.

```{r}
unsolved_plot =
  prob_unsolved |> 
  mutate(city = forcats::fct_reorder(city, estimate)) |> 
ggplot(aes(x = city, y = estimate, fill = city))+
  geom_bar(stat = "identity")+
   geom_errorbar(
    aes(ymin = conf.low, ymax = conf.high),
    position = position_dodge(width = 0.9),
    width = 0.25
  ) +
  labs(title = "Estimated proportion of unsolved murders by city", x = "City", y = "Proportion of murders that are unsolved") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(unsolved_plot)
```


## Problem 2:Experimental data

The following chunk uses a function to read multiple csv files from a zip folder. Each .csv file represents longitudinal data for either a control or experimental group subject.

```{r, message = FALSE}
zip_data =
list.files(path = "./data/data_csv", full.names = TRUE)

#example
subject_data = 
    read_csv("./data/data_csv/con_01.csv") |> 
    janitor::clean_names()
  
# function 
read_function = function(filename) {
  subject_data = 
    read_csv(filename) |> 
    janitor::clean_names()
  return(subject_data)
}

#map

table = map(zip_data, ~ read_function(.x))

all_data = 
  tibble(subject = zip_data, subject_data = table) |> 
  unnest(cols = c(subject_data))

```

The combined dataset containing data for all subjects will be tidied in the following chunk.The subject ID was pulled from the file name, creating a new column called "id". The experimental group was then pulled from the "id," creating a new column called "group". 

```{r}
tidy_data =
  all_data |> 
  mutate(
    id = sub(".csv", "", basename(subject)),
    group = ifelse(str_detect(id, "^exp"), "experimental", "control")) |> 
  pivot_longer(week_1:week_8,
               names_to = "time_point",
               values_to = "observation") |> 
  select(id, group, everything(), -subject)
```

A spaghetti plot is created in the chunk below showing observations on each subject over time by group. This graph shows that the experimental group had greater change in the value of the observation over time, as the value of the observation at week 8 is higher among all experimental group subjects compared to control group, even thought experimental and control group subjects had simillar observation values at baseline.

```{r}
subject_graph =
  tidy_data |> 
  ggplot(aes(x = time_point, y = observation, group = id, color = group))+
  geom_point()+
  geom_line()+
  labs(title = "Changes in observation by subject and group over 8 weeks", x = "Week", y = "Observation") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(subject_graph)
```

## Problem 3: Simulation

```{r}
set.seed(1)

#write a function to define parameters
sim_power = function(n = 30, sigma = 5, mu = 0){
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  
  sim_data |> 
    summarize(
      mu_hat = mean(x),
      sigma_hat = sd(x)
    )
  
  return(sim_data)
}


#create dataset including 5000 iterations of function above
sim_results =
  expand_grid(
    n = 30,
    iter = 1:5000
  ) |> 
  mutate(
    estimate_df = map(n, sim_power)
  )

#write function to run t test. Mu can be changed.
sim_power_repeat = function(mu) {
  df = expand_grid(
    n = 30,
    iter = 1:5000
  ) |> 
  mutate(
    mean = mu,
    estimate_df = map(n, sim_power),
    estimates = map(estimate_df, ~ broom::tidy(t.test(.x, mu = mu)))
  ) |> 
  unnest(c(estimates))
  return(df)
}

#run t test function nfor 7 different mu and combine outputs
sim_results_df =   
  map(0:6, ~ sim_power_repeat(.x)) |> 
  bind_rows()



```

